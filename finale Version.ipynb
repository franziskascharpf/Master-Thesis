{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compatible-signal",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim\n",
    "import gensim, spacy, logging, warnings\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import lemmatize, simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "!#python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-sacramento",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Masterdaten1_BMW.xlsx')\n",
    "data1=data.drop(['Level2', 'Level1'], axis = 1)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-bristol",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = data['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['clean_sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-great",
   "metadata": {},
   "source": [
    "### Data cleaning (with simple process from gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-empty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  \n",
    "\n",
    "# Convert to list\n",
    "data = data.clean_sentence.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-shelf",
   "metadata": {},
   "source": [
    "### bigram and trigram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-niger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# !python3 -m spacy download en  # run in terminal once\n",
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    \n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out\n",
    "\n",
    "data_ready = process_words(data_words)  # processed Text Data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_ready)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-african",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_ready)\n",
    "id2word.filter_extremes(no_below=40,no_above=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-adams",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Corpus: Term Document Frequency\n",
    "corpus_doc2bow = [id2word.doc2bow(text) for text in data_ready]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(corpus_doc2bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_doc2bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = gensim.models.TfidfModel(corpus_doc2bow)\n",
    "corpus_tfidf = tfidf[corpus_doc2bow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-engagement",
   "metadata": {},
   "source": [
    "### LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-expansion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model_tfidf = gensim.models.ldamodel.LdaModel(corpus=corpus_tfidf,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=6, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=10000,\n",
    "                                           passes=10,\n",
    "                                           alpha='symmetric',\n",
    "                                           eta='auto',\n",
    "                                           iterations=50,\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "#print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_tfidf.show_topics(num_words=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-arctic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, id2word, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pyLDAvis.gensim.prepare(lda_model_tfidf, corpus_tfidf, id2word)\n",
    "pyLDAvis.save_html(p, 'finalo6topicsfinal2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-rings",
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_lda = gensim.models.CoherenceModel(model=lda_model_tfidf, texts=data_ready, dictionary=id2word, window_size=60, coherence='c_v')\n",
    "\n",
    "# Calculate and print coherence\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score:', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-backup",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-search",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "CoherenceModel(lda_model_tfidf, texts=data_ready, dictionary=id2word, window_size=60).get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using UMass\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=data_ready, dictionary=id2word, coherence=\"u_mass\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-pathology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score using UMass\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=data_ready, dictionary=id2word, coherence=\"c_uci\")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, window_size, start=2, step=1):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values1 = []\n",
    "    model_list1 = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, window_size=window_size, coherence='u_mass')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list1, coherence_values1 = compute_coherence_values(dictionary=id2word, corpus=corpus_tfidf, texts=data_ready, limit=20, window_size=60, start=2, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, window_size, start=2, step=1):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        \n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        #model = gensim.models.CoherenceModel(model=lda_model_tfidf, texts=data_ready, dictionary=id2word, window_size=60, coherence='c_v')\n",
    "        #model=gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, window_size=60)\n",
    "        \n",
    "        model_list.append(model)\n",
    "        \n",
    "        #coherence_model_lda = gensim.models.CoherenceModel(model=lda_model_tfidf, texts=data_ready, dictionary=id2word, window_size=60, coherence='c_v')\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, window_size=window_size)\n",
    "        \n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus_tfidf, texts=data_ready, limit=20, window_size=60, start=2, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-pizza",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "limit=20; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.title(\"Coherence score tfidf\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "#seem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-approval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coherence values for varying alpha\n",
    "def compute_coherence_values_ALPHA(corpus, dictionary, num_topics, texts, start, limit, step):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for alpha in range(start, limit, step):\n",
    "        model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=seed, alpha=alpha/10, passes=100)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "model_list, coherence_values = compute_coherence_values_ALPHA(dictionary=id2word, corpus=corpus_tfidf, num_topics=num_topics, texts=data_ready, start=1, limit=10, step=1)\n",
    "\n",
    "# Plot graph of coherence values by varying alpha\n",
    "limit=10; start=1; step=1;\n",
    "x_axis = []\n",
    "for x in range(start, limit, step):\n",
    "    x_axis.append(x/10)\n",
    "plt.plot(x_axis, coherence_values)\n",
    "plt.xlabel(\"Alpha\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-diabetes",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus_tfidf,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=10000,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-plant",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 4\n",
    "max_topics = 20\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus_tfidf)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus_tfidf, num_of_docs*0.75), \n",
    "               corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    #iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "         #iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                 iterare through beta values\n",
    "                for b in beta:\n",
    "                     get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "                                                #  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-smith",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-therapist",
   "metadata": {},
   "source": [
    "## Wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Wordcloud of Top N words in each topic\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(stopwords=stop_words,\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model_tfidf.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-subscription",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-carroll",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-white",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = lda_model_tfidf.get_document_topics(corpus_tfidf, minimum_probability=0)\n",
    "all_topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=None, corpus=corpus_tfidf, texts=data,ratings=data, documents=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    \n",
    "      # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    rating = pd.Series(ratings)\n",
    "    docs = pd.Series(documents)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents, rating, documents], axis=1)\n",
    "    return(sent_topics_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['clean_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model_tfidf, corpus=corpus_tfidf, texts=data_ready, ratings=data1['rating'], documents = data1['clean_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text','Rating','Docs']\n",
    "#df_dominant_topic.head(10)\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic.to_excel(r'/Users/franziskascharpf/Desktop/Finalcode/\\Overviewtopics9_Masterdata.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-opportunity",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subjective-bottom",
   "metadata": {},
   "source": [
    "## Embedded projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for tensors(vectors)\n",
    "with open('wcc35LW_lda_tensor.tsv','w') as w:\n",
    "    for doc_topics in all_topics:\n",
    "        for topics in doc_topics:\n",
    "            w.write(str(topics[1])+ \"\\t\")\n",
    "        w.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-economy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for metadata(documet titles)\n",
    "import io\n",
    "import numpy as np\n",
    "with io.open('wcc35LW_lda_metadata.tsv','w', encoding=\"utf-8\") as w:\n",
    "    for doc_id in range(len(all_topics)):\n",
    "        w.write(df_dominant_topic.Dominant_Topic[doc_id].astype(np.str)+\"\\t\" + data.clean_sentence[doc_id] +\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for metadata(documet titles)\n",
    "with open('doc_lda_metadata.tsv','w') as w:\n",
    "    for doc_id in range(len(all_topics)):\n",
    "        w.write(data.clean_sentence[doc_id] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-workshop",
   "metadata": {},
   "source": [
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-shelf",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "from plotly.graph_objs import *\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "py.init_notebook_mode()\n",
    "\n",
    "def plot_difference(mdiff, title=\"\", annotation=None):\n",
    "    \"\"\"\n",
    "    Helper function to plot difference between models\n",
    "    \"\"\"\n",
    "    annotation_html = None\n",
    "    if annotation is not None:\n",
    "        annotation_html = [[\"+++ {}<br>--- {}\".format(\", \".join(int_tokens), \", \".join(diff_tokens))\n",
    "                            for (int_tokens, diff_tokens) in row]\n",
    "                           for row in annotation]\n",
    "        \n",
    "    data = Heatmap(z=mdiff, colorscale='RdBu', text=annotation_html)\n",
    "    layout = Layout(width=950, height=950, title=title,\n",
    "                       xaxis=dict(title=\"topic\"), yaxis=dict(title=\"topic\"))\n",
    "    py.iplot(dict(data=[data], layout=layout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_matrix, annotation = lda_model_tfidf.diff(lda_model_tfidf, distance='jensen_shannon', num_words=50)\n",
    "plot_difference(difference_matrix, title=\"Topic difference [jensen shannon distance]\", annotation=annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_number = 0\n",
    "doc_topic, word_topic, phi_value = lda_model_tfidf.get_document_topics(corpus_tfidf[doc_number], per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-depth",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hidden-surgeon",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-advancement",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-territory",
   "metadata": {},
   "source": [
    "# Clustering K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # To ignore all warnings that arise here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data1.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata = data['clean_sentence'].values\n",
    "type(newdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-brazilian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_ready[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indirect-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "datablock = pd.Series(data_ready)\n",
    "datablock.to_frame()\n",
    "datablock1 = datablock.rename({'0': 'clean_sentence'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu1=datablock1.to_frame()\n",
    "neu1.columns={'clean_sentence'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "black-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-married",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data1.join(neu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-trouble",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu1['clean_sentence'] = [','.join(map(str, l)) for l in neu1['clean_sentence']]\n",
    "neu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-canvas",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-still",
   "metadata": {},
   "outputs": [],
   "source": [
    "#neu1[\"clean_sentence\"] = neu1['clean_sentence'].str.replace([^\\w\\s]' , '   ')\n",
    "neu1[\"clean_sentence\"] = neu1['clean_sentence'].str.replace(',' , ' ')                                                           \n",
    "#neu1['clean_sentence'].str.replace('[{}]'.format(string.punctuation), '  ')\n",
    "#neu1[\"new_column\"] = neu1['clean_sentence'].str.strip()\n",
    "neu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-refrigerator",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu2=neu1.join(data1.rating)\n",
    "neu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virgin-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = neu2['clean_sentence'].to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "principal_components = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neu2.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-underground",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu1['clean_sentence'] = neu1['clean_sentence'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([data1.clean_sentence, neu1, data1.rating], axis=1)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_excel(r'/Users/franziskascharpf/Desktop/Finalcode/\\Vergleich_proprocessing.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(neu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-medline",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array erstellen mit meinen Daten\n",
    "#documents = data1['clean_sentence'].values.astype(\"U\")\n",
    "documents = neu2['clean_sentence'].values.astype(\"U\")\n",
    "#documents = data1['clean_sentence']\n",
    "#documents = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-debate",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vectorizer.fit_transform(documents)\n",
    "print(features)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "newfeatures=features.toarray()\n",
    "#newfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-ocean",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "principal_components = pca.fit_transform(newfeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "y_kmeans = kmeans.fit_predict(principal_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for data points\n",
    "plt.scatter(principal_components[:, 0], principal_components[:, 1], c=y_kmeans, cmap='viridis')\n",
    "\n",
    "# plot for centroids\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "motivated-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativ der gleiche Plot, diesmal mit Cluster Label:\n",
    "\n",
    "# unique clusters\n",
    "u_clusters = np.unique(y_kmeans)\n",
    "\n",
    "# plot for data points\n",
    "for i in u_clusters:\n",
    "    plt.scatter(principal_components[y_kmeans == i, 0], principal_components[y_kmeans == i, 1], label = i)\n",
    "    \n",
    "# plot for centroids\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black')\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-voltage",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psychological-medium",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "plt.spy(features, markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 6\n",
    "model = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=10)\n",
    "model.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "neu2['cluster'] = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-picture",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=neu2['cluster']\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffile = pd.DataFrame(data=file)\n",
    "#dffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffile['sentence']=neu2['clean_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-bottle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffile['rating']=neu2['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffile.to_excel(r'/Users/franziskascharpf/Desktop/Finalcode/\\clusteruebersicht5.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data\n",
    "\n",
    "clusters = neu2.groupby('cluster')    \n",
    "\n",
    "for cluster in clusters.groups:\n",
    "    f = open('cluster'+str(cluster)+ '.csv', 'w') # create csv file\n",
    "    data = clusters.get_group(cluster)[['clean_sentence']] # get title and overview columns\n",
    "    f.write(data.to_csv(index_label='id')) # set index to id\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=clusters)\n",
    "df.head()\n",
    "df.to_excel(r'/Users/franziskascharpf/Desktop/Finalcode/\\clusteruebersicht.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-obligation",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Cluster centroids: \\n\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for j in order_centroids[i, :50]: #print out 10 feature terms of each cluster\n",
    "        print (' %s' % terms[j])\n",
    "    print('------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adequate-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    " print(\"Top terms per cluster:\")\n",
    " order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    " terms = vectorizer.get_feature_names()\n",
    " for i in range(true_k):\n",
    "     print(\"Cluster %d:\" % i, end='')\n",
    "     for ind in order_centroids[i, :10]:\n",
    "         print(' %s' % terms[ind], end='')\n",
    "         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(1,12)\n",
    "\n",
    "#for k in K:\n",
    " #   model = KMeans(n_clusters=k, init='k-means++', max_iter=100, n_init=1)\n",
    "  #  model.fit(features)\n",
    "    #kmeanModel = KMeans(n_clusters=k).fit(order_centroids)\n",
    "    #kmeanModel.fit(order_centroids)\n",
    "   # distortions.append(sum(np.min(cdist(order_centroids, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / order_centroids.shape[0])\n",
    "\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(features)\n",
    "    distortions.append(kmeanModel.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-loading",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "sil = []\n",
    "kmax = 15\n",
    "\n",
    "# dissimilarity would not be defined for a single cluster, thus, minimum number of clusters should be 2\n",
    "for k in range(2, kmax+1):\n",
    "  kmeans = KMeans(n_clusters = k).fit(features)\n",
    "  labels = kmeans.labels_\n",
    "  sil.append(silhouette_score(features, labels, metric = 'euclidean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, sil, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('sil')\n",
    "plt.title('The shilouette method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-tender",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-bradley",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "fig, ax = plt.subplots(figsize = (10,8))\n",
    "data = features\n",
    "kmeans_clustering = KMeans( n_clusters = 6 )\n",
    "idx = kmeans_clustering.fit_predict( data )\n",
    "\n",
    "#use t-sne\n",
    "X = TSNE(n_components=2, perplexity=10).fit_transform( data )\n",
    "\n",
    "#fig = plt.figure(1)\n",
    "#plt.clf()\n",
    "\n",
    "#plot graph\n",
    "colors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n",
    "plt.scatter(X[:,0], X[:,1], c=colors[kmeans_clustering.labels_])\n",
    "plt.title('K-Means (t-SNE)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required modules\n",
    " \n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = features\n",
    "pca = PCA(2)\n",
    " \n",
    "#Transform the data\n",
    "df = pca.fit_transform(data)\n",
    " \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-intervention",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    " \n",
    "#filter rows of original data\n",
    "filtered_label0 = df[label == 0]\n",
    " \n",
    "#plotting the results\n",
    "plt.scatter(filtered_label0[:,0] , filtered_label0[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter rows of original data\n",
    "filtered_label2 = df[label == 2]\n",
    " \n",
    "filtered_label8 = df[label == 8]\n",
    " \n",
    "#Plotting the results\n",
    "plt.scatter(filtered_label2[:,0] , filtered_label2[:,1] , color = 'red')\n",
    "plt.scatter(filtered_label8[:,0] , filtered_label8[:,1] , color = 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = order_centroids\n",
    "pca = PCA(2)\n",
    " \n",
    "#Transform the data\n",
    "#df = pca.fit_transform(data)\n",
    " \n",
    "#Import KMeans module\n",
    "from sklearn.cluster import KMeans\n",
    " \n",
    "#Initialize the class object\n",
    "kmeans = KMeans(n_clusters= 6)\n",
    " \n",
    "#predict the labels of clusters.\n",
    "label = kmeans.fit_predict(data)\n",
    " \n",
    "#Getting unique labels\n",
    "u_labels = np.unique(label)\n",
    " \n",
    "#plotting the results:\n",
    "for i in u_labels:\n",
    "    plt.scatter(data[label == i , 0] , data[label == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transsexual-falls",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Visualising the clusters\n",
    "plt.scatter(documents[model.labels_==0, 0], documents[model.labels_==0, 1], s=100, c='red', label ='Cluster 1')\n",
    "plt.scatter(documents[model.labels_==1, 0], documents[model.labels_==1, 1], s=100, c='blue', label ='Cluster 2')\n",
    "plt.scatter(documents[model.labels_==2, 0], documents[model.labels_==2, 1], s=100, c='green', label ='Cluster 3')\n",
    "plt.scatter(documents[model.labels_==3, 0], documents[model.labels_==3, 1], s=100, c='cyan', label ='Cluster 4')\n",
    "plt.scatter(documents[model.labels_==4, 0], documents[model.labels_==4, 1], s=100, c='magenta', label ='Cluster 5')\n",
    "\n",
    "#Plot the centroid. This time we're going to use the cluster centres  #attribute that returns here the coordinates of the centroid.\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='yellow', label = 'Centroids')\n",
    "plt.title('Clusters of Customers')\n",
    "plt.xlabel('Annual Income(k$)')\n",
    "plt.ylabel('Spending Score(1-100')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-spirituality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = terms\n",
    "pca = PCA(2)\n",
    " \n",
    "#Transform the data\n",
    "#df = pca.fit_transform(data)\n",
    " \n",
    "#Import KMeans module\n",
    "from sklearn.cluster import KMeans\n",
    " \n",
    "#Initialize the class object\n",
    "kmeans = KMeans(n_clusters= 6)\n",
    " \n",
    "#predict the labels of clusters.\n",
    "label = kmeans.fit_predict(data)\n",
    " \n",
    "#Getting unique labels\n",
    "u_labels = np.unique(label)\n",
    " \n",
    "#plotting the results:\n",
    "for i in u_labels:\n",
    "    plt.scatter(data[label == i , 0] , data[label == i , 1] , label = i)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "silhouette_avg = silhouette_score(features, cluster_labels)\n",
    "print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg,)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "sample_silhouette_values = silhouette_samples(features, cluster_labels)\n",
    "\n",
    "y_lower = 10\n",
    "for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "    ith_cluster_silhouette_values.sort()\n",
    "\n",
    "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "    y_upper = y_lower + size_cluster_i\n",
    "\n",
    "    color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "    ax1.fill_betweenx(\n",
    "        np.arange(y_lower, y_upper),\n",
    "        0,\n",
    "        ith_cluster_silhouette_values,\n",
    "        facecolor=color,\n",
    "        edgecolor=color,\n",
    "        alpha=0.7,)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "ax2.scatter(X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\" )\n",
    "\n",
    "    # Labeling the clusters\n",
    "centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "ax2.scatter(centers[:, 0],centers[:, 1], marker=\"o\",c=\"white\",alpha=1, s=200, edgecolor=\"k\",)\n",
    "\n",
    "for i, c in enumerate(centers):\n",
    "    ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle(\"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\" % n_clusters,fontsize=14,fontweight=\"bold\",)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "color-colorado",
   "metadata": {},
   "source": [
    "### Evaluation clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "from sklearn.metrics.cluster import v_measure_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-treat",
   "metadata": {},
   "source": [
    "Homogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneity_score(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-times",
   "metadata": {},
   "source": [
    "Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-failure",
   "metadata": {},
   "source": [
    "V-measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-essence",
   "metadata": {},
   "source": [
    "__________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
