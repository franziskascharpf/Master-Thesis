{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "import pandas as pd\n",
    "\n",
    "#df_terms = pd.read_excel('wordcount.xlsx')\n",
    "df_terms = pd.read_excel('coding2.1 übersicht level1.xlsx')\n",
    "#df_terms = pd.read_excel('test_1.xlsx')\n",
    "\n",
    "#df_terms=df_terms.loc[df_terms['Level1'] == 'Attitude & Behaviour']\n",
    "#df_terms=df_terms.loc[df_terms['brand'] == 'BMW']\n",
    "df_terms = df_terms.reset_index()\n",
    "df_terms=df_terms.drop(['index','brand'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-enclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-terrorism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "example_sent = \"\"\"This is a sample sentence,\n",
    "                  showing off the stop words filtration.\"\"\"\n",
    " \n",
    "stop_words = set(stopwords.words('english'))\n",
    " \n",
    "f = lambda x: ' '.join([item for item in x.split() if item not in stop_words])\n",
    "df_terms[\"clean_sentence\"] = df_terms[\"clean_sentence\"].apply(f)\n",
    "\n",
    "stop_words = [\"\\n\",\"\\\"\"]\n",
    "df_terms[\"clean_sentence\"] = df_terms[\"clean_sentence\"].apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 library\n",
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)\n",
    "\n",
    "def common_start(sa, sb):\n",
    "    \"\"\" returns the longest common substring from the beginning of sa and sb \"\"\"\n",
    "    def _iter():\n",
    "        for a, b in zip(sa, sb):\n",
    "            if a == b:\n",
    "                yield a\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    return ''.join(_iter())\n",
    "\n",
    "def my_common_kernel(sa, sb):\n",
    "\n",
    "    mySubstrings = []\n",
    "    myKernelStrings = []\n",
    "    minSize = int(len(sa)*0.8)\n",
    "    #myString = input(\"Enter a string: \")\n",
    "    \n",
    "    if len(sa)/len(sb)<0.7: return \n",
    "\n",
    "    actualLength = 0\n",
    "    kernelString = \"\"\n",
    "\n",
    "    for i in range(len(sa)-minSize):\n",
    "        mySubstrings.append(sa[0:len(sa)-i])\n",
    "                \n",
    "    for j in range(len(mySubstrings)):\n",
    "        for i in range(len(sb)-minSize+1):\n",
    "            kernelString = common_start(sb[0:len(mySubstrings[j])], mySubstrings[j])\n",
    "            if len(kernelString)>minSize: myKernelStrings.append(kernelString)\n",
    "                \n",
    "    if len(myKernelStrings)>0: \n",
    "        return max(myKernelStrings, key=len)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlling-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategories = df_terms['Level2'].unique()\n",
    "subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-specification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ermittle top terms pro Level2 categorie\n",
    "for i in range(len(subcategories)):\n",
    "    print(subcategories[i])    \n",
    "    df_terms2=df_terms.loc[df_terms['Level2'] == subcategories[i]]\n",
    "    df_terms2 = df_terms2.reset_index(drop=False)\n",
    "    \n",
    "    alle_terms_liste = []\n",
    "    \n",
    "    # separiere alle Wörter aus allen Texten in eine Liste\n",
    "    for j in range(len(df_terms2)):\n",
    "        topic_term_liste = df_terms2.loc[j]['clean_sentence'].split() \n",
    "        alle_terms_liste = alle_terms_liste + topic_term_liste\n",
    "\n",
    "    #myString = input(\"Enter a string: \")\n",
    "    new_terms = []\n",
    "    # entferne alle Zahlen und wörter unter 3 Buchstaben\n",
    "    for k in range(len(alle_terms_liste)):\n",
    "        if len(alle_terms_liste[k]) > 2 and not num_there(alle_terms_liste[k]): \n",
    "            new_terms.append(alle_terms_liste[k])\n",
    "    \n",
    "    lemmatized_List = []\n",
    "    #myString = input(\"Enter a string: \")  \n",
    "    myTerms = new_terms\n",
    "    \n",
    "    while (len(myTerms)>0):\n",
    "\n",
    "        lemmatized_List_tmp = []\n",
    "        myTerms_tmp = []\n",
    "\n",
    "        lemmatized_List_tmp.append([myTerms[0], myTerms[0]])\n",
    "\n",
    "        for word in myTerms[1:]:\n",
    "\n",
    "            myKernel = my_common_kernel(myTerms[0], word)\n",
    "            if myKernel: \n",
    "                lemmatized_List_tmp.append([word, myKernel])\n",
    "            else:\n",
    "                myTerms_tmp.append(word)\n",
    "\n",
    "        columns = ['word','lemma']\n",
    "\n",
    "        df = pd.DataFrame(data=lemmatized_List_tmp,columns=columns)\n",
    "\n",
    "        kernel=min(df[\"lemma\"], key=len)\n",
    "        df['real_lemma'] = kernel\n",
    "        df=df.drop(['lemma'], axis = 1)\n",
    "        lemmatized_List_tmp=df.values.tolist()\n",
    "\n",
    "        lemmatized_List.extend(lemmatized_List_tmp)\n",
    "\n",
    "        myTerms = myTerms_tmp\n",
    "        #print(len(myTerms))\n",
    "\n",
    "    columns = ['word','lemma']\n",
    "    dftmp = pd.DataFrame(data=lemmatized_List,columns=columns)\n",
    "\n",
    "    dff = pd.DataFrame(data=dftmp['lemma'].value_counts())\n",
    "    dff['top_term'] = dff.index\n",
    "    \n",
    "    dff.rename(columns={'lemma': 'count'}, inplace=True)\n",
    "    dff=dff.head(40)\n",
    "    dff = dff[['top_term', 'count']]\n",
    "    dff[subcategories[i]] = dff['top_term'] + \" \" + dff['count'].astype(str)\n",
    "    dff = dff.drop(['top_term', 'count'], axis=1)\n",
    "    dff = dff.reset_index(drop=True)\n",
    "    \n",
    "    if i==0:\n",
    "        columns = [subcategories[i]]\n",
    "        df_full = pd.DataFrame(data=dff[subcategories[i]].tolist(),columns=[subcategories[i]])\n",
    "    else:\n",
    "        df_full[subcategories[i]]=dff[subcategories[i]]\n",
    "    \n",
    "    df_full = df_full.append(dff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_rows', None)\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.to_excel('Attitude&behaviour_topterms_per_subcategory.xlsx')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
